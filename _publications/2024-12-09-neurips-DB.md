---
title: "TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases"
collection: publications
permalink: /publication/db-neurips24
excerpt: 'While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark (https://github.com/serval-uni-lu/tabularbench) where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep learning over 200 models across five critical scenarios in finance, healthcare and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.'
date: 2024-12-09
venue: 'NeurIPS 2024 (BECNHMARK)'
paperurl: 'https://arxiv.org/pdf/2408.07579'
citation: 'Simonetto, T., Ghamizi, S., and Cordy, M. "TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases." arXiv preprint arXiv:2408.07579 (2024).'
---
[Download paper here](https://arxiv.org/pdf/2408.07579)
